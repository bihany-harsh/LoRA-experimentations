{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "ModelHistory = namedtuple(\"ModelHistory\", [\"trainable_params\", \"total_params\", \"results\", \"test_loss\", \"test_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./out_files/MNIST_512_4_FFN_history.pkl\", \"rb\") as f:\n",
    "    vanilla_history = pickle.load(f)\n",
    "with open(\"./out_files/MNIST_512_4_FFN_history_zero_shot.pkl\", \"rb\") as f:\n",
    "    zero_shot_history = pickle.load(f)\n",
    "with open(\"./out_files/MNIST_512_4_FFN_history_finetuned.pkl\", \"rb\") as f:\n",
    "    ft_history = pickle.load(f)\n",
    "with open(\"./out_files/MNIST_512_4_lora_experiment.pkl\", \"rb\") as f:\n",
    "    lora_histories = pickle.load(f)\n",
    "with open(\"./out_files/MNIST_512_4_pre_mult_lora_FFN_history.pkl\", \"rb\") as f:\n",
    "    pre_mult_lora_history = pickle.load(f)\n",
    "with open(\"./out_files/MNIST_512_4_pre_mult_lora_experiment.pkl\", \"rb\") as f:\n",
    "    pre_mult_r_lora_histories = pickle.load(f)\n",
    "with open(\"./out_files/MNIST_512_4_post_mult_lora_FFN_history.pkl\", \"rb\") as f:\n",
    "    post_mult_lora_history = pickle.load(f)\n",
    "with open(\"./out_files/MNIST_512_4_post_mult_lora_experiment.pkl\", \"rb\") as f:\n",
    "    post_mult_r_lora_histories = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'n_original_model_params': [],\n",
    "    'n_finetuning_params': [],\n",
    "    'test_acc (finetuning)': [],\n",
    "    'test_acc (zero_shot)': [],\n",
    "    'n_Full_PreMult_LoRA_params': [],\n",
    "    'test_acc (Full_PreMult_LoRA)': [],\n",
    "    'n_Full_PostMult_LoRA_params': [],\n",
    "    'test_acc (Full_PostMult_LoRA)': [],\n",
    "    'rank': [],\n",
    "    'n_LoRA_params': [],\n",
    "    'test_acc (LoRA r)': [],\n",
    "    'n_PreMult_LoRA_params': [],\n",
    "    'test_acc (PreMult_LoRA r)': [],\n",
    "    'n_PostMult_LoRA_params': [],\n",
    "    'test_acc (PostMult_LoRA r)': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = [2**i for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rank in ranks:\n",
    "    data['n_original_model_params'].append(vanilla_history.trainable_params)\n",
    "    data['n_finetuning_params'].append(ft_history.trainable_params)\n",
    "    data['test_acc (finetuning)'].append(ft_history.test_accuracy)\n",
    "    data['test_acc (zero_shot)'].append(zero_shot_history.test_accuracy)\n",
    "    data['n_Full_PreMult_LoRA_params'].append(pre_mult_lora_history.trainable_params)\n",
    "    data['test_acc (Full_PreMult_LoRA)'].append(pre_mult_lora_history.test_accuracy)\n",
    "    data['n_Full_PostMult_LoRA_params'].append(post_mult_lora_history.trainable_params)\n",
    "    data['test_acc (Full_PostMult_LoRA)'].append(post_mult_lora_history.test_accuracy)\n",
    "\n",
    "    data['rank'].append(rank)\n",
    "    data['n_LoRA_params'].append(lora_histories[rank].trainable_params)\n",
    "    data['test_acc (LoRA r)'].append(lora_histories[rank].test_accuracy)\n",
    "    data['n_PreMult_LoRA_params'].append(pre_mult_r_lora_histories[rank].trainable_params)\n",
    "    data['test_acc (PreMult_LoRA r)'].append(pre_mult_r_lora_histories[rank].test_accuracy)\n",
    "    data['n_PostMult_LoRA_params'].append(post_mult_r_lora_histories[rank].trainable_params)\n",
    "    data['test_acc (PostMult_LoRA r)'].append(post_mult_r_lora_histories[rank].test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_original_model_params</th>\n",
       "      <th>n_finetuning_params</th>\n",
       "      <th>test_acc (finetuning)</th>\n",
       "      <th>test_acc (zero_shot)</th>\n",
       "      <th>n_Full_PreMult_LoRA_params</th>\n",
       "      <th>test_acc (Full_PreMult_LoRA)</th>\n",
       "      <th>n_Full_PostMult_LoRA_params</th>\n",
       "      <th>test_acc (Full_PostMult_LoRA)</th>\n",
       "      <th>rank</th>\n",
       "      <th>n_LoRA_params</th>\n",
       "      <th>test_acc (LoRA r)</th>\n",
       "      <th>n_PreMult_LoRA_params</th>\n",
       "      <th>test_acc (PreMult_LoRA r)</th>\n",
       "      <th>n_PostMult_LoRA_params</th>\n",
       "      <th>test_acc (PostMult_LoRA r)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1460229</td>\n",
       "      <td>1460229</td>\n",
       "      <td>0.979428</td>\n",
       "      <td>0.400535</td>\n",
       "      <td>1925376</td>\n",
       "      <td>0.743057</td>\n",
       "      <td>1310745</td>\n",
       "      <td>0.972845</td>\n",
       "      <td>1</td>\n",
       "      <td>5909</td>\n",
       "      <td>0.911952</td>\n",
       "      <td>6688</td>\n",
       "      <td>0.289241</td>\n",
       "      <td>5130</td>\n",
       "      <td>0.243983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1460229</td>\n",
       "      <td>1460229</td>\n",
       "      <td>0.979428</td>\n",
       "      <td>0.400535</td>\n",
       "      <td>1925376</td>\n",
       "      <td>0.743057</td>\n",
       "      <td>1310745</td>\n",
       "      <td>0.972845</td>\n",
       "      <td>2</td>\n",
       "      <td>11818</td>\n",
       "      <td>0.947542</td>\n",
       "      <td>13376</td>\n",
       "      <td>0.750463</td>\n",
       "      <td>10260</td>\n",
       "      <td>0.804978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1460229</td>\n",
       "      <td>1460229</td>\n",
       "      <td>0.979428</td>\n",
       "      <td>0.400535</td>\n",
       "      <td>1925376</td>\n",
       "      <td>0.743057</td>\n",
       "      <td>1310745</td>\n",
       "      <td>0.972845</td>\n",
       "      <td>4</td>\n",
       "      <td>23636</td>\n",
       "      <td>0.961119</td>\n",
       "      <td>26752</td>\n",
       "      <td>0.916684</td>\n",
       "      <td>20520</td>\n",
       "      <td>0.929233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1460229</td>\n",
       "      <td>1460229</td>\n",
       "      <td>0.979428</td>\n",
       "      <td>0.400535</td>\n",
       "      <td>1925376</td>\n",
       "      <td>0.743057</td>\n",
       "      <td>1310745</td>\n",
       "      <td>0.972845</td>\n",
       "      <td>8</td>\n",
       "      <td>47272</td>\n",
       "      <td>0.967496</td>\n",
       "      <td>53504</td>\n",
       "      <td>0.939519</td>\n",
       "      <td>41040</td>\n",
       "      <td>0.929233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1460229</td>\n",
       "      <td>1460229</td>\n",
       "      <td>0.979428</td>\n",
       "      <td>0.400535</td>\n",
       "      <td>1925376</td>\n",
       "      <td>0.743057</td>\n",
       "      <td>1310745</td>\n",
       "      <td>0.972845</td>\n",
       "      <td>16</td>\n",
       "      <td>94544</td>\n",
       "      <td>0.976754</td>\n",
       "      <td>107008</td>\n",
       "      <td>0.940547</td>\n",
       "      <td>82080</td>\n",
       "      <td>0.938901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1460229</td>\n",
       "      <td>1460229</td>\n",
       "      <td>0.979428</td>\n",
       "      <td>0.400535</td>\n",
       "      <td>1925376</td>\n",
       "      <td>0.743057</td>\n",
       "      <td>1310745</td>\n",
       "      <td>0.972845</td>\n",
       "      <td>32</td>\n",
       "      <td>189088</td>\n",
       "      <td>0.978194</td>\n",
       "      <td>214016</td>\n",
       "      <td>0.936021</td>\n",
       "      <td>164160</td>\n",
       "      <td>0.722074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1460229</td>\n",
       "      <td>1460229</td>\n",
       "      <td>0.979428</td>\n",
       "      <td>0.400535</td>\n",
       "      <td>1925376</td>\n",
       "      <td>0.743057</td>\n",
       "      <td>1310745</td>\n",
       "      <td>0.972845</td>\n",
       "      <td>64</td>\n",
       "      <td>378176</td>\n",
       "      <td>0.976342</td>\n",
       "      <td>428032</td>\n",
       "      <td>0.929644</td>\n",
       "      <td>328320</td>\n",
       "      <td>0.234725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1460229</td>\n",
       "      <td>1460229</td>\n",
       "      <td>0.979428</td>\n",
       "      <td>0.400535</td>\n",
       "      <td>1925376</td>\n",
       "      <td>0.743057</td>\n",
       "      <td>1310745</td>\n",
       "      <td>0.972845</td>\n",
       "      <td>128</td>\n",
       "      <td>756352</td>\n",
       "      <td>0.971611</td>\n",
       "      <td>856064</td>\n",
       "      <td>0.911952</td>\n",
       "      <td>656640</td>\n",
       "      <td>0.268463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1460229</td>\n",
       "      <td>1460229</td>\n",
       "      <td>0.979428</td>\n",
       "      <td>0.400535</td>\n",
       "      <td>1925376</td>\n",
       "      <td>0.743057</td>\n",
       "      <td>1310745</td>\n",
       "      <td>0.972845</td>\n",
       "      <td>256</td>\n",
       "      <td>1512704</td>\n",
       "      <td>0.956593</td>\n",
       "      <td>1712128</td>\n",
       "      <td>0.887883</td>\n",
       "      <td>1313280</td>\n",
       "      <td>0.200370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1460229</td>\n",
       "      <td>1460229</td>\n",
       "      <td>0.979428</td>\n",
       "      <td>0.400535</td>\n",
       "      <td>1925376</td>\n",
       "      <td>0.743057</td>\n",
       "      <td>1310745</td>\n",
       "      <td>0.972845</td>\n",
       "      <td>512</td>\n",
       "      <td>3025408</td>\n",
       "      <td>0.932524</td>\n",
       "      <td>3424256</td>\n",
       "      <td>0.211479</td>\n",
       "      <td>2626560</td>\n",
       "      <td>0.207570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_original_model_params  n_finetuning_params  test_acc (finetuning)  \\\n",
       "0                  1460229              1460229               0.979428   \n",
       "1                  1460229              1460229               0.979428   \n",
       "2                  1460229              1460229               0.979428   \n",
       "3                  1460229              1460229               0.979428   \n",
       "4                  1460229              1460229               0.979428   \n",
       "5                  1460229              1460229               0.979428   \n",
       "6                  1460229              1460229               0.979428   \n",
       "7                  1460229              1460229               0.979428   \n",
       "8                  1460229              1460229               0.979428   \n",
       "9                  1460229              1460229               0.979428   \n",
       "\n",
       "   test_acc (zero_shot)  n_Full_PreMult_LoRA_params  \\\n",
       "0              0.400535                     1925376   \n",
       "1              0.400535                     1925376   \n",
       "2              0.400535                     1925376   \n",
       "3              0.400535                     1925376   \n",
       "4              0.400535                     1925376   \n",
       "5              0.400535                     1925376   \n",
       "6              0.400535                     1925376   \n",
       "7              0.400535                     1925376   \n",
       "8              0.400535                     1925376   \n",
       "9              0.400535                     1925376   \n",
       "\n",
       "   test_acc (Full_PreMult_LoRA)  n_Full_PostMult_LoRA_params  \\\n",
       "0                      0.743057                      1310745   \n",
       "1                      0.743057                      1310745   \n",
       "2                      0.743057                      1310745   \n",
       "3                      0.743057                      1310745   \n",
       "4                      0.743057                      1310745   \n",
       "5                      0.743057                      1310745   \n",
       "6                      0.743057                      1310745   \n",
       "7                      0.743057                      1310745   \n",
       "8                      0.743057                      1310745   \n",
       "9                      0.743057                      1310745   \n",
       "\n",
       "   test_acc (Full_PostMult_LoRA)  rank  n_LoRA_params  test_acc (LoRA r)  \\\n",
       "0                       0.972845     1           5909           0.911952   \n",
       "1                       0.972845     2          11818           0.947542   \n",
       "2                       0.972845     4          23636           0.961119   \n",
       "3                       0.972845     8          47272           0.967496   \n",
       "4                       0.972845    16          94544           0.976754   \n",
       "5                       0.972845    32         189088           0.978194   \n",
       "6                       0.972845    64         378176           0.976342   \n",
       "7                       0.972845   128         756352           0.971611   \n",
       "8                       0.972845   256        1512704           0.956593   \n",
       "9                       0.972845   512        3025408           0.932524   \n",
       "\n",
       "   n_PreMult_LoRA_params  test_acc (PreMult_LoRA r)  n_PostMult_LoRA_params  \\\n",
       "0                   6688                   0.289241                    5130   \n",
       "1                  13376                   0.750463                   10260   \n",
       "2                  26752                   0.916684                   20520   \n",
       "3                  53504                   0.939519                   41040   \n",
       "4                 107008                   0.940547                   82080   \n",
       "5                 214016                   0.936021                  164160   \n",
       "6                 428032                   0.929644                  328320   \n",
       "7                 856064                   0.911952                  656640   \n",
       "8                1712128                   0.887883                 1313280   \n",
       "9                3424256                   0.211479                 2626560   \n",
       "\n",
       "   test_acc (PostMult_LoRA r)  \n",
       "0                    0.243983  \n",
       "1                    0.804978  \n",
       "2                    0.929233  \n",
       "3                    0.929233  \n",
       "4                    0.938901  \n",
       "5                    0.722074  \n",
       "6                    0.234725  \n",
       "7                    0.268463  \n",
       "8                    0.200370  \n",
       "9                    0.207570  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
