{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "M = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn([N,M])\n",
    "W2 = torch.randn([N,M])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_pre = torch.nn.Parameter(torch.eye(N))\n",
    "A_post = torch.nn.Parameter(torch.eye(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam([A_pre], lr = 1e-3)\n",
    "criteria = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9945, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4378e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.2493e-12, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.8200e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4240e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.9684e-10, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.0537e-09, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8333e-11, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.8016e-09, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    optimiser.zero_grad()\n",
    "    loss = criteria(W2, A_pre @ W1)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    if i%1000==0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam([A_post], lr = 1e-3)\n",
    "criteria = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9945, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5985, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4943, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4883, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4882, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4882, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4882, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4882, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4882, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4882, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    optimiser.zero_grad()\n",
    "    loss = criteria(W2, W1 @ A_post)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    if i%1000==0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lora_matrices(A, B):\n",
    "    N,R = A.shape\n",
    "    A.requires_grad = False\n",
    "    B.requires_grad = False\n",
    "    B[0][0] = 1\n",
    "    A[:,0] = 1\n",
    "    A.requires_grad = True\n",
    "    B.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift(M):\n",
    "    n_rows, n_cols = M.shape\n",
    "    arange2 = torch.arange(n_cols).view(( 1,n_cols)).repeat((n_rows,1))\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            arange2[i][j] = ((i - j) % n_cols)\n",
    "    return torch.gather(M, 1, arange2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A_pre_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Rank Post 1 #####\n",
      "tensor(0.6557, grad_fn=<MseLossBackward0>)\n",
      "##### Rank Post 2 #####\n",
      "tensor(0.3853, grad_fn=<MseLossBackward0>)\n",
      "##### Rank Post 4 #####\n",
      "tensor(0.1144, grad_fn=<MseLossBackward0>)\n",
      "##### Rank Post 8 #####\n",
      "tensor(1.7161e-07, grad_fn=<MseLossBackward0>)\n",
      "##### Rank Post 16 #####\n",
      "tensor(4.8441e-07, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "R = 1\n",
    "while R <= N:\n",
    "    print(\"#\"*5 + \" Rank Post \" + str(R) + \" \" + \"#\"*5)\n",
    "    A1 = torch.nn.Parameter(torch.randn([N,R]))\n",
    "    B1 = torch.nn.Parameter(torch.zeros([R,N]))\n",
    "    init_lora_matrices(A1,B1)\n",
    "    optimiser = torch.optim.Adam([A1, B1], lr = 1e-3)\n",
    "    criteria = torch.nn.MSELoss()\n",
    "    for i in range(10000):\n",
    "        optimiser.zero_grad()\n",
    "        loss = criteria(W2, shift(A1 @ B1) @ W1)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    print(loss)\n",
    "    R=R*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A_post_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Rank Pre 1 #####\n",
      "tensor(0.8304, grad_fn=<MseLossBackward0>)\n",
      "##### Rank Pre 2 #####\n",
      "tensor(0.6946, grad_fn=<MseLossBackward0>)\n",
      "##### Rank Pre 4 #####\n",
      "tensor(0.5611, grad_fn=<MseLossBackward0>)\n",
      "##### Rank Pre 8 #####\n",
      "tensor(0.4909, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "R = 1\n",
    "while R <= M:\n",
    "    print(\"#\"*5 + \" Rank Pre \" + str(R) + \" \" + \"#\"*5)\n",
    "    A1 = torch.nn.Parameter(torch.randn([M,R]))\n",
    "    B1 = torch.nn.Parameter(torch.zeros([R,M]))\n",
    "    init_lora_matrices(A1,B1)\n",
    "    optimiser = torch.optim.Adam([A1, B1], lr = 1e-3)\n",
    "    criteria = torch.nn.MSELoss()\n",
    "    for i in range(10000):\n",
    "        optimiser.zero_grad()\n",
    "        loss = criteria(W2, W1 @ shift(A1 @ B1))\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    print(loss)\n",
    "    R=R*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
